{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fsx/phuc/temp/unit_mup/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import *\n",
    "\n",
    "import datasets\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor, tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import unit_scaling as uu\n",
    "import unit_scaling.functional as U\n",
    "\n",
    "# Config & helpers\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 256\n",
    "depth = 4\n",
    "head_size = 64\n",
    "mlp_expansion = 2\n",
    "\n",
    "# Training\n",
    "n_steps = int(5e3)\n",
    "warmup_steps = int(1e3)\n",
    "batch_size = 16\n",
    "sequence_length = 256\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "compile = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|████████████████████████████████████████████████████████████████████| 10.5k/10.5k [00:00<00:00, 45.0MB/s]\n",
      "Downloading data: 100%|████████████████████████████████████████████████████████████████████████| 733k/733k [00:00<00:00, 21.2MB/s]\n",
      "Downloading data: 100%|█████████████████████████████████████████████████████████████████████████| 157M/157M [00:00<00:00, 376MB/s]\n",
      "Downloading data: 100%|█████████████████████████████████████████████████████████████████████████| 157M/157M [00:00<00:00, 320MB/s]\n",
      "Downloading data: 100%|████████████████████████████████████████████████████████████████████████| 657k/657k [00:00<00:00, 26.8MB/s]\n",
      "Generating test split: 100%|████████████████████████████████████████████████████████| 4358/4358 [00:00<00:00, 64378.44 examples/s]\n",
      "Generating train split: 100%|████████████████████████████████████████████████| 1801350/1801350 [00:01<00:00, 975090.72 examples/s]\n",
      "Generating validation split: 100%|█████████████████████████████████████████████████| 3760/3760 [00:00<00:00, 577888.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"Salesforce/wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "data = torch.frombuffer(bytearray(\"\".join(dataset[\"text\"]), encoding=\"utf8\"), dtype=torch.uint8)\n",
    "def batches() -> Iterable[Tensor]:\n",
    "    for _ in range(n_steps):\n",
    "        yield torch.stack([\n",
    "            data[i:i + sequence_length].to(device=device, dtype=torch.long)\n",
    "            for i in torch.randint(0, len(data) - sequence_length, size=(batch_size,))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpTransformerLayer(nn.Module):\n",
    "    def __init__(self, width: int) -> None:\n",
    "        super().__init__()\n",
    "        self.attn_norm = nn.LayerNorm(width, elementwise_affine=False)\n",
    "        self.attn_qkv = nn.Linear(width, 3 * width, bias=False)\n",
    "        self.attn_out = nn.Linear(width, width, bias=False)\n",
    "\n",
    "        self.mlp_norm = nn.LayerNorm(width, elementwise_affine=False)\n",
    "        self.mlp_up = nn.Linear(width, mlp_expansion * width, bias=False)\n",
    "        self.mlp_gate = nn.Linear(width, mlp_expansion * width, bias=False)\n",
    "        self.mlp_down = nn.Linear(mlp_expansion * width, width, bias=False)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        residual = self.attn_norm(input)\n",
    "        q, k, v = einops.rearrange(self.attn_qkv(residual), \"b s (z h d) -> z b h s d\", d=head_size, z=3)\n",
    "        qkv = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        residual = self.attn_out(einops.rearrange(qkv, \"b h s d -> b s (h d)\"))\n",
    "        input = input + residual\n",
    "\n",
    "        residual = self.mlp_norm(input)\n",
    "        residual = self.mlp_down(self.mlp_up(residual) * F.silu(self.mlp_gate(residual)))\n",
    "        return input + residual\n",
    "\n",
    "\n",
    "class SpTransformerDecoder(nn.Sequential):\n",
    "    def __init__(self, width: int) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, width)\n",
    "        self.layers = nn.Sequential(*(SpTransformerLayer(width) for _ in range(depth)))\n",
    "        self.final_norm = nn.LayerNorm(width, elementwise_affine=False)\n",
    "        self.projection = nn.Linear(width, vocab_size, bias=False)\n",
    "\n",
    "    def loss(self, input_ids: Tensor) -> Tensor:\n",
    "        logits = self(input_ids).float()\n",
    "        return F.cross_entropy(\n",
    "            logits[..., :-1, :].flatten(end_dim=-2), input_ids[..., 1:].flatten()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# SpTransformerLayer:\n",
      "              output.std = 1.01\n",
      "          input.grad.std = 1.01\n",
      "     attn_qkv.weight.std = 0.05\n",
      "     attn_out.weight.std = 0.05\n",
      "       mlp_up.weight.std = 0.05\n",
      "     mlp_gate.weight.std = 0.05\n",
      "     mlp_down.weight.std = 0.04\n",
      "attn_qkv.weight.grad.std = 3.88\n",
      "attn_out.weight.grad.std = 6.06\n",
      "  mlp_up.weight.grad.std = 8.27\n",
      "mlp_gate.weight.grad.std = 8.51\n",
      "mlp_down.weight.grad.std = 11.79\n"
     ]
    }
   ],
   "source": [
    "def show_layer_stats(layer: nn.Module, input_shape: Tuple[int, ...]) -> None:\n",
    "    input = torch.randn(*input_shape, requires_grad=True)\n",
    "    output = layer(input)\n",
    "    output.backward(torch.randn_like(output))\n",
    "    print(f\"# {type(layer).__name__}:\")\n",
    "    for k, v in {\n",
    "        \"output\": output.std(),\n",
    "        \"input.grad\": input.grad.std(),\n",
    "        **{f\"{name}\": param.std() for name, param in layer.named_parameters()},\n",
    "        **{f\"{name}.grad\": param.grad.std() for name, param in layer.named_parameters()},\n",
    "    }.items():\n",
    "        print(f\"{k:>20}.std = {v.item():.2f}\")\n",
    "\n",
    "show_layer_stats(SpTransformerLayer(128), (batch_size, sequence_length, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UmupTransformerLayer(nn.Module):\n",
    "    def __init__(self, width: int, layer_idx: int) -> None:\n",
    "        super().__init__()\n",
    "        self.attn_norm = uu.LayerNorm(width)\n",
    "        self.attn_qkv = uu.Linear(width, 3 * width)\n",
    "        self.attn_out = uu.Linear(width, width)\n",
    "\n",
    "        self.mlp_norm = uu.LayerNorm(width)\n",
    "        self.mlp_up = uu.Linear(width, mlp_expansion * width)\n",
    "        self.mlp_gate = uu.Linear(width, mlp_expansion * width)\n",
    "        self.mlp_down = uu.Linear(mlp_expansion * width, width)\n",
    "\n",
    "        tau_rule = uu.transformer_residual_scaling_rule()\n",
    "        self.attn_tau = tau_rule(2 * layer_idx, 2 * depth)\n",
    "        self.mlp_tau = tau_rule(2 * layer_idx + 1, 2 * depth)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        residual, skip = U.residual_split(input, self.attn_tau)\n",
    "        residual = self.attn_norm(residual)\n",
    "        q, k, v = einops.rearrange(self.attn_qkv(residual), \"b s (z h d) -> z b h s d\", d=head_size, z=3)\n",
    "        qkv = U.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        residual = self.attn_out(einops.rearrange(qkv, \"b h s d -> b s (h d)\"))\n",
    "        input = U.residual_add(residual, skip, self.attn_tau)\n",
    "\n",
    "        residual, skip = U.residual_split(input, self.mlp_tau)\n",
    "        residual = self.mlp_norm(residual)\n",
    "        residual = self.mlp_down(U.silu_glu(self.mlp_up(residual), self.mlp_gate(residual)))\n",
    "        return U.residual_add(residual, skip, self.mlp_tau)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UmupTransformerDecoder(nn.Sequential):\n",
    "    def __init__(self, width: int) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = uu.Embedding(vocab_size, width)\n",
    "        self.layers = uu.DepthSequential(*(UmupTransformerLayer(width, i) for i in range(depth)))\n",
    "        self.final_norm = uu.LayerNorm(width)\n",
    "        self.projection = uu.LinearReadout(width, vocab_size)\n",
    "\n",
    "    def loss(self, input_ids: Tensor) -> Tensor:\n",
    "        logits = self(input_ids).float()\n",
    "        return U.cross_entropy(\n",
    "            logits[..., :-1, :].flatten(end_dim=-2), input_ids[..., 1:].flatten()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# UmupTransformerLayer:\n",
      "              output.std = 1.01\n",
      "          input.grad.std = 1.10\n",
      "     attn_qkv.weight.std = 1.00\n",
      "     attn_out.weight.std = 1.00\n",
      "       mlp_up.weight.std = 1.00\n",
      "     mlp_gate.weight.std = 1.00\n",
      "     mlp_down.weight.std = 1.00\n",
      "attn_qkv.weight.grad.std = 0.63\n",
      "attn_out.weight.grad.std = 1.07\n",
      "  mlp_up.weight.grad.std = 0.70\n",
      "mlp_gate.weight.grad.std = 0.73\n",
      "mlp_down.weight.grad.std = 1.00\n"
     ]
    }
   ],
   "source": [
    "show_layer_stats(UmupTransformerLayer(128, 0), (batch_size, sequence_length, 128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
