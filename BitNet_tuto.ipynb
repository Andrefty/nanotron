{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0yoL6yyv1oU"
   },
   "source": [
    "## What is BitNet?\n",
    "\n",
    "[BitNet](https://arxiv.org/abs/2402.17764) replaces traditional Linear layers in Multi-Head Attention and Feed-Forward Networks with specialized layers called BitLinear with ternary (or binary in the older version) precision. The BitLinear layers introduce in this notebook quantize the weights using ternary precision (with values of -1, 0, and 1) and quantize the activations to 8-bit precision.\n",
    "\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/bitlinear.png\" alt=\"Alt Text\" />\n",
    "  <figcaption>The architecture of BitNet with BitLinear layers</figcaption>\n",
    "</figure>\n",
    "\n",
    "It's worth mentioning that the behavior of BitLinear differs between training and inference. For example, during training, we start by quantizing the weights into ternary values, using symmetric per tensor quantization. First, we compute the average of the absolute values of the weight matrix and use this as a scale. We then divide the weights by the scale, round the values, constrain them between -1 and 1, and finally rescale them to continue in full precision.\n",
    "\n",
    "$$\n",
    "scale_w = \\frac{1}{\\frac{1}{nm} \\sum_{ij} |W_{ij}|}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_q = \\text{clamp}_{[-1,1]}(\\text{round}(W*scale))\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_{dequantized} = W_q*scale_w\n",
    "$$\n",
    "\n",
    "Activations are then quantized to a specified bit-width (e.g., 8-bit) using [absmax](https://arxiv.org/pdf/2208.07339) quantization (symmetric per channel quantization). This involves scaling the activations into a range [âˆ’128,127[. The quantization formula is:\n",
    "\n",
    "$$\n",
    "scale_x = \\frac{127}{|X|_{\\text{max}, \\, \\text{dim}=-1}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X_q = \\text{clamp}_{[-128,127]}(\\text{round}(X*scale))\n",
    "$$\n",
    "\n",
    "$$\n",
    "X_{dequantized} = X_q * scale_x\n",
    "$$\n",
    "\n",
    "The main obstacle to training in ternary precision is that the weight values are discretized (via the `round()` function) and thus non-differentiable. BitLinear solves this with a nice trick: [STE (Straight Through Estimator)](https://arxiv.org/abs/1903.05662). The STE allows gradients to flow through the non-differentiable rounding operation by approximating its gradient as 1 (treating `round()` as equivalent to the identity function). Another way to view it is that, instead of stopping the gradient at the rounding step, the STE lets the gradient pass through as if the rounding never occurred, enabling weight updates using standard gradient-based optimization techniques.\n",
    "\n",
    "To learn more about how we trained, and fine-tuned bitnet models checkout the blogpost [here](https://)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Af_mMRJJxhdo"
   },
   "source": [
    "## How to load bitnet models from the hub ?\n",
    "\n",
    "Models in ternary precision are packed with 2 bits per weight. You can load them directly using from_pretrained, provided that the quantization method is specified as bitnet in the config.json.\n",
    "\n",
    "Start by changing the runtime to use GPUs, and follow the next steps :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kv79bhZnlnQZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreif/.conda/envs/nanotronenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBuHVXdgz14x",
    "outputId": "8e17d93d-fffb-43fb-f201-b1b4994ae80d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWhat is the capital of France?\\nParis is the capital of France'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\"/home/andreif/Documents/nanotron/models/Unpacked-Llama3-Bitnet\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/home/andreif/Documents/nanotron/models/Llama3-8B-1.58-100B-tokens\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/andreif/Documents/nanotron/models/Llama3-8B-1.58-100B-tokens\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "input_text = \"\"\"\n",
    "What is the capital of France?\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").cuda()\n",
    "output = model.generate(input_ids, max_new_tokens=6)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"user\\n\\nWrite a command using wfuzz to fuzz this website: 'https://github.com/unslothai/unsloth/issues/278'assistant\\n\\n-0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"/home/andreif/Documents/nanotron/models/Llama3-8B-1.58-100B-tokens\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/andreif/Documents/nanotron/models/Llama3-8B-1.58-100B-tokens\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create chat messages in the format expected by the model\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Write a command using wfuzz to fuzz this website: 'https://github.com/unslothai/unsloth/issues/278'\"}\n",
    "]\n",
    "\n",
    "# Use the model's built-in template\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").cuda()\n",
    "output = model.generate(input_ids, max_new_tokens=100)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With model unpacked (bitnet_weight_unpack.py), then converted to nanotron, then back to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/andreif/Documents/nanotron/hf_checkpoints/Packed-Llama3-8B-Bitnet were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight_packed', 'model.layers.0.mlp.gate_proj.weight_packed', 'model.layers.0.mlp.up_proj.weight_packed', 'model.layers.0.self_attn.k_proj.weight_packed', 'model.layers.0.self_attn.o_proj.weight_packed', 'model.layers.0.self_attn.q_proj.weight_packed', 'model.layers.0.self_attn.v_proj.weight_packed', 'model.layers.1.mlp.down_proj.weight_packed', 'model.layers.1.mlp.gate_proj.weight_packed', 'model.layers.1.mlp.up_proj.weight_packed', 'model.layers.1.self_attn.k_proj.weight_packed', 'model.layers.1.self_attn.o_proj.weight_packed', 'model.layers.1.self_attn.q_proj.weight_packed', 'model.layers.1.self_attn.v_proj.weight_packed', 'model.layers.10.mlp.down_proj.weight_packed', 'model.layers.10.mlp.gate_proj.weight_packed', 'model.layers.10.mlp.up_proj.weight_packed', 'model.layers.10.self_attn.k_proj.weight_packed', 'model.layers.10.self_attn.o_proj.weight_packed', 'model.layers.10.self_attn.q_proj.weight_packed', 'model.layers.10.self_attn.v_proj.weight_packed', 'model.layers.11.mlp.down_proj.weight_packed', 'model.layers.11.mlp.gate_proj.weight_packed', 'model.layers.11.mlp.up_proj.weight_packed', 'model.layers.11.self_attn.k_proj.weight_packed', 'model.layers.11.self_attn.o_proj.weight_packed', 'model.layers.11.self_attn.q_proj.weight_packed', 'model.layers.11.self_attn.v_proj.weight_packed', 'model.layers.12.mlp.down_proj.weight_packed', 'model.layers.12.mlp.gate_proj.weight_packed', 'model.layers.12.mlp.up_proj.weight_packed', 'model.layers.12.self_attn.k_proj.weight_packed', 'model.layers.12.self_attn.o_proj.weight_packed', 'model.layers.12.self_attn.q_proj.weight_packed', 'model.layers.12.self_attn.v_proj.weight_packed', 'model.layers.13.mlp.down_proj.weight_packed', 'model.layers.13.mlp.gate_proj.weight_packed', 'model.layers.13.mlp.up_proj.weight_packed', 'model.layers.13.self_attn.k_proj.weight_packed', 'model.layers.13.self_attn.o_proj.weight_packed', 'model.layers.13.self_attn.q_proj.weight_packed', 'model.layers.13.self_attn.v_proj.weight_packed', 'model.layers.14.mlp.down_proj.weight_packed', 'model.layers.14.mlp.gate_proj.weight_packed', 'model.layers.14.mlp.up_proj.weight_packed', 'model.layers.14.self_attn.k_proj.weight_packed', 'model.layers.14.self_attn.o_proj.weight_packed', 'model.layers.14.self_attn.q_proj.weight_packed', 'model.layers.14.self_attn.v_proj.weight_packed', 'model.layers.15.mlp.down_proj.weight_packed', 'model.layers.15.mlp.gate_proj.weight_packed', 'model.layers.15.mlp.up_proj.weight_packed', 'model.layers.15.self_attn.k_proj.weight_packed', 'model.layers.15.self_attn.o_proj.weight_packed', 'model.layers.15.self_attn.q_proj.weight_packed', 'model.layers.15.self_attn.v_proj.weight_packed', 'model.layers.16.mlp.down_proj.weight_packed', 'model.layers.16.mlp.gate_proj.weight_packed', 'model.layers.16.mlp.up_proj.weight_packed', 'model.layers.16.self_attn.k_proj.weight_packed', 'model.layers.16.self_attn.o_proj.weight_packed', 'model.layers.16.self_attn.q_proj.weight_packed', 'model.layers.16.self_attn.v_proj.weight_packed', 'model.layers.17.mlp.down_proj.weight_packed', 'model.layers.17.mlp.gate_proj.weight_packed', 'model.layers.17.mlp.up_proj.weight_packed', 'model.layers.17.self_attn.k_proj.weight_packed', 'model.layers.17.self_attn.o_proj.weight_packed', 'model.layers.17.self_attn.q_proj.weight_packed', 'model.layers.17.self_attn.v_proj.weight_packed', 'model.layers.18.mlp.down_proj.weight_packed', 'model.layers.18.mlp.gate_proj.weight_packed', 'model.layers.18.mlp.up_proj.weight_packed', 'model.layers.18.self_attn.k_proj.weight_packed', 'model.layers.18.self_attn.o_proj.weight_packed', 'model.layers.18.self_attn.q_proj.weight_packed', 'model.layers.18.self_attn.v_proj.weight_packed', 'model.layers.19.mlp.down_proj.weight_packed', 'model.layers.19.mlp.gate_proj.weight_packed', 'model.layers.19.mlp.up_proj.weight_packed', 'model.layers.19.self_attn.k_proj.weight_packed', 'model.layers.19.self_attn.o_proj.weight_packed', 'model.layers.19.self_attn.q_proj.weight_packed', 'model.layers.19.self_attn.v_proj.weight_packed', 'model.layers.2.mlp.down_proj.weight_packed', 'model.layers.2.mlp.gate_proj.weight_packed', 'model.layers.2.mlp.up_proj.weight_packed', 'model.layers.2.self_attn.k_proj.weight_packed', 'model.layers.2.self_attn.o_proj.weight_packed', 'model.layers.2.self_attn.q_proj.weight_packed', 'model.layers.2.self_attn.v_proj.weight_packed', 'model.layers.20.mlp.down_proj.weight_packed', 'model.layers.20.mlp.gate_proj.weight_packed', 'model.layers.20.mlp.up_proj.weight_packed', 'model.layers.20.self_attn.k_proj.weight_packed', 'model.layers.20.self_attn.o_proj.weight_packed', 'model.layers.20.self_attn.q_proj.weight_packed', 'model.layers.20.self_attn.v_proj.weight_packed', 'model.layers.21.mlp.down_proj.weight_packed', 'model.layers.21.mlp.gate_proj.weight_packed', 'model.layers.21.mlp.up_proj.weight_packed', 'model.layers.21.self_attn.k_proj.weight_packed', 'model.layers.21.self_attn.o_proj.weight_packed', 'model.layers.21.self_attn.q_proj.weight_packed', 'model.layers.21.self_attn.v_proj.weight_packed', 'model.layers.22.mlp.down_proj.weight_packed', 'model.layers.22.mlp.gate_proj.weight_packed', 'model.layers.22.mlp.up_proj.weight_packed', 'model.layers.22.self_attn.k_proj.weight_packed', 'model.layers.22.self_attn.o_proj.weight_packed', 'model.layers.22.self_attn.q_proj.weight_packed', 'model.layers.22.self_attn.v_proj.weight_packed', 'model.layers.23.mlp.down_proj.weight_packed', 'model.layers.23.mlp.gate_proj.weight_packed', 'model.layers.23.mlp.up_proj.weight_packed', 'model.layers.23.self_attn.k_proj.weight_packed', 'model.layers.23.self_attn.o_proj.weight_packed', 'model.layers.23.self_attn.q_proj.weight_packed', 'model.layers.23.self_attn.v_proj.weight_packed', 'model.layers.24.mlp.down_proj.weight_packed', 'model.layers.24.mlp.gate_proj.weight_packed', 'model.layers.24.mlp.up_proj.weight_packed', 'model.layers.24.self_attn.k_proj.weight_packed', 'model.layers.24.self_attn.o_proj.weight_packed', 'model.layers.24.self_attn.q_proj.weight_packed', 'model.layers.24.self_attn.v_proj.weight_packed', 'model.layers.25.mlp.down_proj.weight_packed', 'model.layers.25.mlp.gate_proj.weight_packed', 'model.layers.25.mlp.up_proj.weight_packed', 'model.layers.25.self_attn.k_proj.weight_packed', 'model.layers.25.self_attn.o_proj.weight_packed', 'model.layers.25.self_attn.q_proj.weight_packed', 'model.layers.25.self_attn.v_proj.weight_packed', 'model.layers.26.mlp.down_proj.weight_packed', 'model.layers.26.mlp.gate_proj.weight_packed', 'model.layers.26.mlp.up_proj.weight_packed', 'model.layers.26.self_attn.k_proj.weight_packed', 'model.layers.26.self_attn.o_proj.weight_packed', 'model.layers.26.self_attn.q_proj.weight_packed', 'model.layers.26.self_attn.v_proj.weight_packed', 'model.layers.27.mlp.down_proj.weight_packed', 'model.layers.27.mlp.gate_proj.weight_packed', 'model.layers.27.mlp.up_proj.weight_packed', 'model.layers.27.self_attn.k_proj.weight_packed', 'model.layers.27.self_attn.o_proj.weight_packed', 'model.layers.27.self_attn.q_proj.weight_packed', 'model.layers.27.self_attn.v_proj.weight_packed', 'model.layers.28.mlp.down_proj.weight_packed', 'model.layers.28.mlp.gate_proj.weight_packed', 'model.layers.28.mlp.up_proj.weight_packed', 'model.layers.28.self_attn.k_proj.weight_packed', 'model.layers.28.self_attn.o_proj.weight_packed', 'model.layers.28.self_attn.q_proj.weight_packed', 'model.layers.28.self_attn.v_proj.weight_packed', 'model.layers.29.mlp.down_proj.weight_packed', 'model.layers.29.mlp.gate_proj.weight_packed', 'model.layers.29.mlp.up_proj.weight_packed', 'model.layers.29.self_attn.k_proj.weight_packed', 'model.layers.29.self_attn.o_proj.weight_packed', 'model.layers.29.self_attn.q_proj.weight_packed', 'model.layers.29.self_attn.v_proj.weight_packed', 'model.layers.3.mlp.down_proj.weight_packed', 'model.layers.3.mlp.gate_proj.weight_packed', 'model.layers.3.mlp.up_proj.weight_packed', 'model.layers.3.self_attn.k_proj.weight_packed', 'model.layers.3.self_attn.o_proj.weight_packed', 'model.layers.3.self_attn.q_proj.weight_packed', 'model.layers.3.self_attn.v_proj.weight_packed', 'model.layers.30.mlp.down_proj.weight_packed', 'model.layers.30.mlp.gate_proj.weight_packed', 'model.layers.30.mlp.up_proj.weight_packed', 'model.layers.30.self_attn.k_proj.weight_packed', 'model.layers.30.self_attn.o_proj.weight_packed', 'model.layers.30.self_attn.q_proj.weight_packed', 'model.layers.30.self_attn.v_proj.weight_packed', 'model.layers.31.mlp.down_proj.weight_packed', 'model.layers.31.mlp.gate_proj.weight_packed', 'model.layers.31.mlp.up_proj.weight_packed', 'model.layers.31.self_attn.k_proj.weight_packed', 'model.layers.31.self_attn.o_proj.weight_packed', 'model.layers.31.self_attn.q_proj.weight_packed', 'model.layers.31.self_attn.v_proj.weight_packed', 'model.layers.4.mlp.down_proj.weight_packed', 'model.layers.4.mlp.gate_proj.weight_packed', 'model.layers.4.mlp.up_proj.weight_packed', 'model.layers.4.self_attn.k_proj.weight_packed', 'model.layers.4.self_attn.o_proj.weight_packed', 'model.layers.4.self_attn.q_proj.weight_packed', 'model.layers.4.self_attn.v_proj.weight_packed', 'model.layers.5.mlp.down_proj.weight_packed', 'model.layers.5.mlp.gate_proj.weight_packed', 'model.layers.5.mlp.up_proj.weight_packed', 'model.layers.5.self_attn.k_proj.weight_packed', 'model.layers.5.self_attn.o_proj.weight_packed', 'model.layers.5.self_attn.q_proj.weight_packed', 'model.layers.5.self_attn.v_proj.weight_packed', 'model.layers.6.mlp.down_proj.weight_packed', 'model.layers.6.mlp.gate_proj.weight_packed', 'model.layers.6.mlp.up_proj.weight_packed', 'model.layers.6.self_attn.k_proj.weight_packed', 'model.layers.6.self_attn.o_proj.weight_packed', 'model.layers.6.self_attn.q_proj.weight_packed', 'model.layers.6.self_attn.v_proj.weight_packed', 'model.layers.7.mlp.down_proj.weight_packed', 'model.layers.7.mlp.gate_proj.weight_packed', 'model.layers.7.mlp.up_proj.weight_packed', 'model.layers.7.self_attn.k_proj.weight_packed', 'model.layers.7.self_attn.o_proj.weight_packed', 'model.layers.7.self_attn.q_proj.weight_packed', 'model.layers.7.self_attn.v_proj.weight_packed', 'model.layers.8.mlp.down_proj.weight_packed', 'model.layers.8.mlp.gate_proj.weight_packed', 'model.layers.8.mlp.up_proj.weight_packed', 'model.layers.8.self_attn.k_proj.weight_packed', 'model.layers.8.self_attn.o_proj.weight_packed', 'model.layers.8.self_attn.q_proj.weight_packed', 'model.layers.8.self_attn.v_proj.weight_packed', 'model.layers.9.mlp.down_proj.weight_packed', 'model.layers.9.mlp.gate_proj.weight_packed', 'model.layers.9.mlp.up_proj.weight_packed', 'model.layers.9.self_attn.k_proj.weight_packed', 'model.layers.9.self_attn.o_proj.weight_packed', 'model.layers.9.self_attn.q_proj.weight_packed', 'model.layers.9.self_attn.v_proj.weight_packed']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/andreif/Documents/nanotron/hf_checkpoints/Packed-Llama3-8B-Bitnet and are newly initialized: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWhat is the capital of France?\\nimersimersimersimersimersimersimersimersimersimersimersimersimersimersimersimersimersimersimersimers'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"/home/andreif/Documents/nanotron/hf_checkpoints/Packed-Llama3-8B-Bitnet\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/home/andreif/Documents/nanotron/hf_checkpoints/Converted-Nanotron-Llama-3-8B-Bitnet\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/andreif/Documents/nanotron/models/Llama3-8B-1.58-100B-tokens\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "input_text = \"\"\"\n",
    "What is the capital of France?\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").cuda()\n",
    "output = model.generate(input_ids, max_new_tokens=20)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "generated_text"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
