##### Running reference #####


[Ref Adam] original grad: tensor([[-5.7679,  5.2140],
        [-5.7679,  5.2140]], device='cuda:0')

[Ref Adam] original exp_avg: exp_avg.data=tensor([[0., 0.],
        [0., 0.]], device='cuda:0'), exp_avg.dtype=torch.float32

[Ref Adam] original exp_avg_sq: exp_avg_sq.data=tensor([[0., 0.],
        [0., 0.]], device='cuda:0'), exp_avg_sq.dtype=torch.float32

[Ref Adam] beta1: 0.9, beta2: 0.999
[Ref Adam]: bias_correction1: 0.09999999999999998, bias_correction2: 0.0010000000000000009
[Ref Adam] after mul and add: exp_avg: tensor([[-0.5768,  0.5214],
        [-0.5768,  0.5214]], device='cuda:0')

[Ref Adam] after mul and add: exp_avg_sq: tensor([[0.0333, 0.0272],
        [0.0333, 0.0272]], device='cuda:0')

[Ref Adam] exp_avg_sq.sqrt(): tensor([[0.1824, 0.1649],
        [0.1824, 0.1649]], device='cuda:0')

[Ref Adam] math.sqrt(bias_correction2)): 0.031622776601683805

[Ref Adam] group['eps']: 1e-08

[Ref Adam] step_size: 0.010000000000000002

[Ref Adam] exp_avg: tensor([[-0.5768,  0.5214],
        [-0.5768,  0.5214]], device='cuda:0')

[Ref Adam] denom: tensor([[5.7679, 5.2140],
        [5.7679, 5.2140]], device='cuda:0')

[Ref Adam] updated p: tensor([[-0.1047, -0.0822],
        [-0.2134, -0.1752]], device='cuda:0')

##### Running MSAMP #####


[MSAMP] grad_lp: tensor([[243, 115],
        [243, 115]], device='cuda:0', dtype=torch.uint8)
[MSAMP] param_lp: tensor([[-27696., -21296.],
        [-56192., -45664.]], device='cuda:0', dtype=torch.float16)
[MSAMP] exp_avg_lp: tensor([[0, 0],
        [0, 0]], device='cuda:0', dtype=torch.uint8)
[MSAMP] exp_avg_sq_lp: tensor([[0., 0.],
        [0., 0.]], device='cuda:0', dtype=torch.float16)
[MSAMP] step: tensor([[243, 115],
        [243, 115]], device='cuda:0', dtype=torch.uint8)
[MSAMP] grad: tensor([[-5.5000,  5.5000],
        [-5.5000,  5.5000]], device='cuda:0')
[MSAMP] param: tensor([[-0.1057, -0.0812],
        [-0.2144, -0.1742]], device='cuda:0')
[MSAMP] param after weight_decay: tensor([[-0.1057, -0.0812],
        [-0.2144, -0.1742]], device='cuda:0')
[MSAMP] exp_avg: tensor([[0., 0.],
        [0., 0.]], device='cuda:0')
[MSAMP] exp_avg_sq: tensor([[0., 0.],
        [0., 0.]], device='cuda:0')
[MSAMP] bias_correction1: 0.09999999999999998
[MSAMP] bias_correction2: 0.0010000000000000009
[MSAMP] step_size: 0.010000000000000002
[MSAMP] exp_avg optimizer updates: tensor([[-0.5500,  0.5500],
        [-0.5500,  0.5500]], device='cuda:0')
[MSAMP] exp_avg_sq optimizer updates: tensor([[0.0303, 0.0303],
        [0.0303, 0.0303]], device='cuda:0')
[MSAMP] denom optimizer updates: tensor([[5.5000, 5.5000],
        [5.5000, 5.5000]], device='cuda:0')
[MSAMP] param optimizer updates: tensor([[-0.1047, -0.0822],
        [-0.2134, -0.1752]], device='cuda:0')
##### Running FP8 #####


[FP8Adam] original grad: tensor([[-5.7679,  5.2140],
        [-5.7679,  5.2140]], device='cuda:0')
[FP8Adam] beta1: 0.9, beta2: 0.999
[FP8Adam]: bias_correction1: 0.09999999999999998, bias_correction2: 0.0010000000000000009
[FP8Adam] original fp8 exp_avg: exp_avg.data=FP8Tensor([[0, 0],
           [0, 0]], device='cuda:0', dtype=torch.uint8), exp_avg.fp8_meta=FP8Meta(amax=0.0, scale=1.0, inverse_scale=1.0, dtype=DTypes.FP8E4M3)

[FP8Adam] exp_avg_fp32: tensor([[0., 0.],
        [0., 0.]], device='cuda:0')

[FP8Adam] exp_avg_sq: tensor([[0., 0.],
        [0., 0.]], device='cuda:0')

[FP8Adam] after mul and add: exp_avg_fp32: tensor([[-0.5768,  0.5214],
        [-0.5768,  0.5214]], device='cuda:0')

[FP8Adam] after mul and add: exp_avg_sq: tensor([[0.0333, 0.0272],
        [0.0333, 0.0272]], device='cuda:0')

[FP8Adam] denom: tensor([[5.7679, 5.2140],
        [5.7679, 5.2140]], device='cuda:0')

[FP8Adam] step_size: 0.010000000000000002

[FP8Adam] updated_exp_avg_fp8: updated_exp_avg_fp8.data=tensor([[241, 112],
        [241, 112]], device='cuda:0', dtype=torch.uint8), exp_avg_fp32_meta=FP8Meta(amax=0.8939014673233032, scale=256.0, inverse_scale=0.00390625, dtype=DTypes.FP8E4M3)

[FP8Adam] updated p_fp32: tensor([[-0.1084, -0.0791],
        [-0.2177, -0.1729]], device='cuda:0')

[FP8Adam] updated_p_fp8: updated_p_fp8.data=tensor([[238, 234],
        [246, 243]], device='cuda:0', dtype=torch.uint8), p_fp32_meta=FP8Meta(amax=0.25099998712539673, scale=1024.0, inverse_scale=0.0009765625, dtype=DTypes.FP8E4M3)
